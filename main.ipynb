{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Learning : Knowing why customers leaves</h1>\n",
    "\n",
    "<p>In this tutorial, we will learn how to use Deep Learning to know why the customers of the bank are leaving.<br>\n",
    "This bank,measured some things is that the customers leaves at unusually rates, and they want to understand what the problem is and they want to assess and adress the problem.\n",
    "This dataset contains relevant informations of customers. It's a record of 10000 transactions in the past months that contains the estimated salary.\n",
    "The column <i>Exited</i> show us if the customers leaves or not, so it's equal 1 for leaving and 0 for staying.\n",
    "</p>\n",
    "<p>The main task here is to predict if the customer will leave so the columns equal 1 or if he will stay in the bank and 0 instead.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So let's get deep here in the dataset. We have 14 columns, we will see each one if we can consider it as a valuable information for our model or not.<br><br>\n",
    "<b>RowNumber</b> and <b>CustomerId</b> respresent a unique ID for each row and customer. Of course this has no impact on the prediciton.<br>\n",
    "<b>Surname</b>Same for the Surname, if your name is Onio it's doesn't mean that you have more chance to leave the bank than Mitchell.<br>\n",
    "<b>CreditScore</b> the credit score is very likely to have an impact on the customer's decision to stay or not, if we think about it, we might expect that customers with a low credit score are more likely to leave the bank than the customers with a high credit.<br>\n",
    "<b>Geography</b> the country might be a valuable information to predict the decision<br>\n",
    "<b>Gender</b> Yes, maybe the men are more likely the leave the bank than women<br>\n",
    "<b>Age</b>Of course, young people mayben are more likely to leave the bank than the old one, due to advantages or taxes or whatever.<br>\n",
    "<b>Tenure</b> is how long the customers are staying on the bank and of course this has an impact on the final decision, maybe with the more years there is a lot of advantages with bank so the customers stay and not leave<br>\n",
    "<b>Balance</b> Same, a customers with 0 Balance are more likely to leave than a customer with high balance.<br>\n",
    "<b>NumOfProducts</b> We never know, maybe and maybe not.<br>\n",
    "<b>HasCrCard</b> The customer with credit card are more likely to stay than who doesn't.<br>\n",
    "<b>IsActiveMember</b> Same for HasCrCard.<br>\n",
    "<b>EstimatedSalary</b> Same logic for Balance, customers with high estimated salary are more likely to stay than with low one.<br>\n",
    "</p>\n",
    "<p>We have listed all the columns and our intuitions, but in reality, we don't know which independant variable has the most impact on the dependant variable (the one that we wil predict). And that's what our Artificial Neural Network will spot.</p>\n",
    "<p>For now, we will not include <b>RowNumber</b> and <b>CustomerId</b> on the model.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the dependant variable and independant variable\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dealing with categorical variable</h3>\n",
    "<p>To run the algorithm, we have to encode the categorical variable. Here we have 2 ones : <b>Geography</b> and <b>Gender</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 0, 0, ..., 1, 1, 101348.88],\n",
       "       [608, 2, 0, ..., 0, 1, 112542.58],\n",
       "       [502, 0, 0, ..., 1, 0, 113931.57],\n",
       "       ..., \n",
       "       [709, 0, 0, ..., 0, 1, 42085.58],\n",
       "       [772, 1, 1, ..., 1, 0, 92888.52],\n",
       "       [792, 0, 0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# For the geography variable\n",
    "labelencoder_geo = LabelEncoder()\n",
    "X[:, 1] = labelencoder_geo.fit_transform(X[:, 1])\n",
    "# For the gender variable\n",
    "labelencoder_gender = LabelEncoder()\n",
    "X[:, 2] = labelencoder_gender.fit_transform(X[:, 2])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The two columns are converted to integer, for the geography variable, each country take one number : France for 0, Germany for 1 and Spain for 2.<br>\n",
    "The same for gender, 0 for female and 1 for male (this is purely random).</p>\n",
    "<p>However, if we let this, the algorithm will consider that if France for 0 and Spain for 2, than Spain is greated and more valuable than France which is not correct, these categorical value are nominal, so there is no order between them. In order to deal with this, we will use the dummy variable</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.01348880e+05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   1.12542580e+05],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   0.00000000e+00,   1.13931570e+05],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   4.20855800e+04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   0.00000000e+00,   9.28885200e+04],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   0.00000000e+00,   3.81907800e+04]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove one dummy variable category to not fall into the dummy variable trap\n",
    "X = X[:, 1:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Due to the high computation, We should apply Feature Scaling to ease the computation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5698444 ,  1.74309049,  0.16958176, ...,  0.64259497,\n",
       "        -1.03227043,  1.10643166],\n",
       "       [ 1.75486502, -0.57369368, -2.30455945, ...,  0.64259497,\n",
       "         0.9687384 , -0.74866447],\n",
       "       [-0.5698444 , -0.57369368, -1.19119591, ...,  0.64259497,\n",
       "        -1.03227043,  1.48533467],\n",
       "       ..., \n",
       "       [-0.5698444 , -0.57369368,  0.9015152 , ...,  0.64259497,\n",
       "        -1.03227043,  1.41231994],\n",
       "       [-0.5698444 ,  1.74309049, -0.62420521, ...,  0.64259497,\n",
       "         0.9687384 ,  0.84432121],\n",
       "       [ 1.75486502, -0.57369368, -0.28401079, ...,  0.64259497,\n",
       "        -1.03227043,  0.32472465]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating the ANN model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "# For creating the Layers\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, the first step of creating our Artifical Neural Network is initializing the end, it's defined by a sequence of layers.<br>\n",
    "Actually, there is two ways to initializing a deep learning model, it's either by <i>defining the sequence of layers</i> or the other way <i>defining a graph</i>. Here, since we will make an artificial neural network with successive layers so we will use the first method.<br>\n",
    "From the Sequential module, we will create object of it. And since, our model will be a classifier (predict if the customer will leave or not), so we will call it \"classifier\".\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, we will add the first layers of our ANN, for this step we will use Dense function.<br><br>\n",
    "In advance, we already know the number of nodes in the input layers and this number is nothing else than the number of independant variable and in this case is 11.<br><br>\n",
    "For the propagation, from the left to right the neurons are activated by the activation function is such a way that the higher the value of the activation fucntion is for the neuron the more impact this neuron is going to have in the network. The activation function which will have to do in this case to define the first hidden layers is the rectifier function, and for the output layers, the sigmoid function is also a good choice since it's a classification case.<br><br>\n",
    "The result of the sigmoid activation function the result in the output layer, will be a probabilities of the customer choice for leaving or not, to greated the probability, the decision of customer to leave is more affirmative. We can segment the cutomers according to their probablity to leave the bank, and according to the terms of business constraints and business goals we will make decisions to add value to the business.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "# Why we choose hidden layer ? Well some says that is called simple \"Art\", and like any Art, by practice and experiment we will\n",
    "# Develope our intuition, or there is a little rule is : (Input layers + Outputs layers) / 2\n",
    "classifier.add(Dense(output_dim = 6, init = \"uniform\", activation = \"relu\", input_dim = 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Adding the second hidden layers\n",
    "# For this layers, we will delete the input layers that's because simple we have already set the input layers\n",
    "classifier.add(Dense(output_dim = 6, init = \"uniform\", activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Adding the output layers\n",
    "# For this layers, we will use the sigmoid function\n",
    "# If we have an output results that consists of many categories, we use the function Softmax\n",
    "classifier.add(Dense(output_dim = 1, init = \"uniform\", activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the steps Above, we have created the ANN and the weights but It's just an initalization step. The compilation consists of the application of the algorithm that will find the optimal set of weights in the whole ANN.<br>\n",
    "For this method, there are 3 parameter :<br>\n",
    "- First, we specity the <i>algorithm of optimisation</i>. We choose the stochastic algorithm, there are several types of stochastic gradient method, we choose Adam.<br>\n",
    "- The seconde parameter is the <i>loss</i> function.<br>\n",
    "- The last parameter is <i>metrics</i>, it's is the creteria for evaluate our algorithm.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss =\"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1230/8000 [===>..........................] - ETA: 0s - loss: 0.3883 - acc: 0.8390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.4005 - acc: 0.8356\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 0.4003 - acc: 0.8359\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.4001 - acc: 0.8357\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 0.4004 - acc: 0.8355 0s - loss: 0.3984 -\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 0.3999 - acc: 0.8351\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.4005 - acc: 0.8352\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 0.4002 - acc: 0.8360\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.4002 - acc: 0.8340\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 0.3999 - acc: 0.8354\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 0.4002 - acc: 0.8362\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.4005 - acc: 0.8346\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.4005 - acc: 0.8359\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.4004 - acc: 0.8357\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.4004 - acc: 0.8360\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.4001 - acc: 0.8352\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.4003 - acc: 0.8364\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 0.4004 - acc: 0.8365\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.4004 - acc: 0.8342\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.4000 - acc: 0.8352 0s - loss: 0.3998 - \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3999 - acc: 0.8352\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3995 - acc: 0.8355\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.4003 - acc: 0.8372\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.4005 - acc: 0.8355\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.4001 - acc: 0.8342\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 0.4006 - acc: 0.8360\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.4001 - acc: 0.8356\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4000 - acc: 0.8361\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.4002 - acc: 0.8349\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 149us/step - loss: 0.4004 - acc: 0.8364\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.4002 - acc: 0.8359\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.4004 - acc: 0.8356\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.4001 - acc: 0.8349\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.4000 - acc: 0.8345\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.4001 - acc: 0.8344\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.4000 - acc: 0.8362\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 0.4004 - acc: 0.8369\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.4002 - acc: 0.8352\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 0.4002 - acc: 0.8356\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4003 - acc: 0.8372\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.3999 - acc: 0.8369\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 0.4001 - acc: 0.8370\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4002 - acc: 0.8349\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3999 - acc: 0.8356\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4003 - acc: 0.8362\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.3999 - acc: 0.8351\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.4000 - acc: 0.8364\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.4001 - acc: 0.8351\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4003 - acc: 0.8349\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.3998 - acc: 0.8345\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.4001 - acc: 0.8364\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.3998 - acc: 0.8356\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.4000 - acc: 0.8349\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4001 - acc: 0.8374\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.3999 - acc: 0.8359\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.3997 - acc: 0.8340\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4000 - acc: 0.8356\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4000 - acc: 0.8351\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.3999 - acc: 0.8346\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.4001 - acc: 0.8356\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.3989 - acc: 0.8351\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4006 - acc: 0.8359\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.4001 - acc: 0.8359\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.3999 - acc: 0.8365\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4002 - acc: 0.8350\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 0.4000 - acc: 0.8350\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.4001 - acc: 0.8354\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.4000 - acc: 0.8349\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 0.4002 - acc: 0.8362\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4003 - acc: 0.8342\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.4000 - acc: 0.8347\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4002 - acc: 0.8356\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4003 - acc: 0.8361\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.3998 - acc: 0.8342\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.3997 - acc: 0.8364\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 0.4001 - acc: 0.8340\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 0.4001 - acc: 0.8351\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 0.4001 - acc: 0.8342\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.3999 - acc: 0.8356\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.4000 - acc: 0.8345\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.4000 - acc: 0.8341\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4001 - acc: 0.8350\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4002 - acc: 0.8361\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4002 - acc: 0.8350\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.4005 - acc: 0.8350\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.4001 - acc: 0.8366\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 0.3999 - acc: 0.8346\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.4000 - acc: 0.8352\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 0.4003 - acc: 0.8356\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4000 - acc: 0.8364\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.3997 - acc: 0.8347\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.3991 - acc: 0.8365\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4000 - acc: 0.8361\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.4000 - acc: 0.8356\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.3999 - acc: 0.8351\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 0.4001 - acc: 0.8350\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.4005 - acc: 0.8347\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.3999 - acc: 0.8355\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.4003 - acc: 0.8336\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.4001 - acc: 0.8357\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.3998 - acc: 0.8355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2687865f4e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the dataset to the ANN\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18717389],\n",
       "       [ 0.32767144],\n",
       "       [ 0.14578097],\n",
       "       ..., \n",
       "       [ 0.14106224],\n",
       "       [ 0.15789446],\n",
       "       [ 0.11409371]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training our ANN on the train set, we will predict the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The result is a value of float, like we said in the beginning, The output will be a probablities of customers'leaving the bank.<br>\n",
    "If we take the first customer, the value is 0.18 so the probablity of this customer for leaving the bank is 0.18.<br>\n",
    "We will convert this values to a decision, by puttin a threashold of 0.5. If the value is equal or greater of the threashold, then the customers will Leave, else he will stay.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ..., \n",
       "       [False],\n",
       "       [False],\n",
       "       [False]], dtype=bool)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_pred > 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1543,   52],\n",
       "       [ 265,  140]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The confusion matrix show us that the prediction consists of 1543 + 140 correct predictions and 52 + 265 incorrect predictions, let's get the accuracy of the prediction</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy :  0.8415\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy : \", (1543 + 140) / (2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Like we say in the beginning, We can sort the probablities from highest to the lowest to get the customers most likely to leave the bank, for example tha bank can look to the 20 % highest probablities of their customers to leave the bank, and make it a segment and then analyzed in more depth to take measers and prevent more customers from leaving.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
